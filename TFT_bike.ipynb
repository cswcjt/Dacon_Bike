{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cswcjt/Dacon_Bike/blob/main/TFT_bike.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezZOqxCoVWtP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pytorch_forecasting\n",
        "!pip install holidays\n",
        "!pip install statsmodels --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEswgAbYWChc",
        "outputId": "8bf4e535-5245-4186-e2f6-eaa70dc2b750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9coFLVyJ0N_b"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch\n",
        "\n",
        "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf \n",
        "import tensorboard as tb \n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juBNbizOV9tX"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "import holidays\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMBvsGHyVafM"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/fastcamp/datas/bike/\"\n",
        "train = pd.read_csv(base_path + \"train.csv\")\n",
        "submission = pd.read_csv(base_path + \"sample_submission.csv\")\n",
        "save_path = \"/content/drive/MyDrive/fastcamp/datas/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0fEY8uoV3k0"
      },
      "outputs": [],
      "source": [
        "train['일시'] = pd.to_datetime(train['일시'], format='%Y%m%d')\n",
        "train = train.rename(columns={\"일시\": \"date\", \"광진구\": \"g\", \"동대문구\": \"d\", \"성동구\": \"s\", \"중랑구\": \"j\"})\n",
        "display((train.merge((train[['date']].drop_duplicates(ignore_index=True).rename_axis('time_idx')).reset_index(), on = ['date'])))\n",
        "train = train.set_index(keys=['date'], drop=True)\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgJ_bJFM0xfS"
      },
      "outputs": [],
      "source": [
        "# data = (train.merge((train[['date']].drop_duplicates(ignore_index=True).rename_axis('time_idx')).reset_index(), on = ['date']))\n",
        "# data\n",
        "# holidays.Korea(years = 2019)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMPDcF5oY_2v"
      },
      "outputs": [],
      "source": [
        "# 데이터의 시작 시점을 기억\n",
        "earliest_time = train.index.min() \n",
        "earliest_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glpX_aUnZD_Q"
      },
      "outputs": [],
      "source": [
        "df_list = []\n",
        "\n",
        "for region in train:\n",
        "    # label = region\n",
        "    ts = train[region]\n",
        "    # display(ts)\n",
        "    \n",
        "    # tmp 생성\n",
        "    # date도 생성 -> month, day, hour 등 시간에 관련 된 모든 데이터 갖고있다.\n",
        "    tmp = pd.DataFrame({'usage': ts})\n",
        "    date = tmp.index\n",
        "\n",
        "    # 시간 변화 대한 정보들 컬럼들도 만들어 준다.  \n",
        "    tmp['days_from_start'] = (date - earliest_time).days\n",
        "    tmp['date'] = date\n",
        "\n",
        "    # 시간 자체에 대한 정보들 컬럼들도 만들어 준다.  \n",
        "    tmp['date'] = date\n",
        "    tmp['day'] = date.day.astype(str).astype(\"category\")\n",
        "    tmp['day_of_week'] = date.dayofweek.astype(str).astype(\"category\")\n",
        "    tmp[\"week_of_year\"] = date.weekofyear.astype(str).astype(\"category\") \n",
        "    tmp['month'] = date.month.astype(str).astype(\"category\")\n",
        "    tmp['year'] = date.year.astype(str).astype(\"category\")\n",
        "\n",
        "    # 고객정보 컬럼도 만들어준다. \n",
        "    tmp['region'] = region\n",
        "\n",
        "    # stack all time series vertically\n",
        "    df_list.append(tmp)\n",
        "\n",
        "demand_df = pd.concat(df_list).reset_index(drop=True)\n",
        "demand_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruNciJCz293s"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "#batch size=64\n",
        "#number heads=4, hidden sizes=160, lr=0.001, gr_clip=0.1\n",
        "\n",
        "# 1년 사용량 예측\n",
        "max_prediction_length = 334\n",
        "\n",
        "# 1년을 lookback window 로 설정: 1*365\n",
        "max_encoder_length = 365*3\n",
        "\n",
        "# train data 설정\n",
        "training_cutoff = demand_df[\"days_from_start\"].max() - max_prediction_length\n",
        "\n",
        "training = TimeSeriesDataSet(\n",
        "    # train dataframe 설정\n",
        "    # idx 설정\n",
        "    demand_df[lambda x: x.days_from_start <= training_cutoff],\n",
        "    time_idx=\"days_from_start\",\n",
        "\n",
        "    # target 설정\n",
        "    target=\"usage\",\n",
        "\n",
        "    # 그룹 설정\n",
        "    group_ids=[\"region\"],\n",
        "\n",
        "    # encoder, prediciton\n",
        "    min_encoder_length=max_encoder_length//2, \n",
        "    max_encoder_length=max_encoder_length,\n",
        "    min_prediction_length=30,\n",
        "    max_prediction_length=max_prediction_length,\n",
        "\n",
        "    # 각 컬럼이 어떤 유형의 변수인지 구분\n",
        "    static_categoricals=[\"region\"],\n",
        "    time_varying_known_categoricals=[\"day\",\"day_of_week\", \"week_of_year\", \"month\", \"year\"],\n",
        "    time_varying_known_reals=[\"days_from_start\"],\n",
        "    time_varying_unknown_reals=['usage'],\n",
        "\n",
        "    add_relative_time_idx=True,\n",
        "    add_target_scales=True,\n",
        "    add_encoder_length=True,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq7KeU6m4CSn"
      },
      "outputs": [],
      "source": [
        "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
        "# for each series\n",
        "validation = TimeSeriesDataSet.from_dataset(training, demand_df, predict=True, stop_randomization=True)\n",
        "\n",
        "# create dataloaders for model\n",
        "batch_size = 128  # set this between 32 to 128\n",
        "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
        "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjLDbLZh-SjL"
      },
      "outputs": [],
      "source": [
        "training.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UZMlUDW43GD"
      },
      "outputs": [],
      "source": [
        "validation.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzUJEecu-MMN"
      },
      "outputs": [],
      "source": [
        "#let's see how a naive model does\n",
        "\n",
        "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
        "baseline_predictions = Baseline().predict(val_dataloader)\n",
        "(actuals - baseline_predictions).abs().mean().item()\n",
        "\n",
        "sm = MAE()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHmFdNSd-rfW"
      },
      "outputs": [],
      "source": [
        "print(f\"Median loss for naive prediction on validation: {sm.loss(actuals, baseline_predictions).mean(axis = 1).median().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0K8sDCz-ufm"
      },
      "outputs": [],
      "source": [
        "PATIENCE = 30\n",
        "MAX_EPOCHS = 120\n",
        "LEARNING_RATE = 0.03\n",
        "OPTUNA = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDG8birk-xYH"
      },
      "outputs": [],
      "source": [
        "early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=1e-2, patience=PATIENCE, verbose=False, mode=\"min\")\n",
        "lr_logger = LearningRateMonitor()  # log the learning rate\n",
        "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=MAX_EPOCHS,\n",
        "    gpus=1,\n",
        "    devices=1, accelerator=\"gpu\",\n",
        "    enable_model_summary=True,\n",
        "    gradient_clip_val=0.25,\n",
        "    limit_train_batches=10,  # coment in for training, running valiation every 30 batches\n",
        "    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
        "    callbacks=[lr_logger, early_stop_callback],\n",
        "    logger=logger,\n",
        ")\n",
        "\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    training,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lstm_layers=2,\n",
        "    hidden_size=16,\n",
        "    attention_head_size=2,\n",
        "    dropout=0.2,\n",
        "    hidden_continuous_size=8,\n",
        "    output_size=1,  # 7 quantiles by default\n",
        "    loss=MAE(),\n",
        "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
        "    reduce_on_plateau_patience=4\n",
        ")\n",
        "\n",
        "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDCoR7SB_MdL"
      },
      "outputs": [],
      "source": [
        "trainer.fit(\n",
        "    tft,\n",
        "    train_dataloaders=train_dataloader,\n",
        "    val_dataloaders=val_dataloader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFgBad9S_cv6"
      },
      "outputs": [],
      "source": [
        "if OPTUNA:\n",
        "\n",
        "    from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
        "\n",
        "    # create study\n",
        "    study = optimize_hyperparameters(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        model_path=\"optuna_test\",\n",
        "        n_trials=50,\n",
        "        max_epochs=50,\n",
        "        gradient_clip_val_range=(0.01, 1.0),\n",
        "        hidden_size_range=(8, 128),\n",
        "        hidden_continuous_size_range=(8, 128),\n",
        "        attention_head_size_range=(1, 4),\n",
        "        learning_rate_range=(0.001, 0.1),\n",
        "        dropout_range=(0.1, 0.3),\n",
        "        trainer_kwargs=dict(limit_train_batches=30),\n",
        "        reduce_on_plateau_patience=4,\n",
        "        use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDTa6k2qC0-z"
      },
      "outputs": [],
      "source": [
        "best_model_path = trainer.checkpoint_callback.best_model_path\n",
        "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
        "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
        "predictions = best_tft.predict(val_dataloader, mode=\"prediction\")\n",
        "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
        "\n",
        "sm = MAE()\n",
        "print(f\"Validation median MAE loss: {sm.loss(actuals, predictions).mean(axis = 1).median().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdXDx_23IhHW"
      },
      "outputs": [],
      "source": [
        "print(f\"Validation median MAE loss: {sm.loss(actuals, predictions).mean(axis = 1).median().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlgydb9BIhXU"
      },
      "outputs": [],
      "source": [
        "for idx in range(raw_predictions.prediction.shape[0]):\n",
        "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOXPzYOIIlo0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpj50wkcsuTJnZiY+WGV/Y",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}